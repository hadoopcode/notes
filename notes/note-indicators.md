####**flink实时指标**
- 订单支付检测 
``` 
思路：使用CEP去匹配，根据订单id分组，从创建订单的事件开始到时间段结束之间，是否支付了，用侧标签来输出超时的
```
- 实时热门商品统计/热门商品页面（topN问题）<br>
```
思路：从kafka中消费到数据，经过etl，获取拿到想要的数据，并封装成bean
    1. 按求最多的字段做分组
    2. 开窗聚合（增量聚合就做求和，全窗口聚合返回[目标字段，度量值，窗口结束时间]）
    3. 按窗口结束时间分组
    4. keyedProcessFunction中process对列表状态变量添加元素，再注册定时器，在定时器中对列表状态变量排序。
```
- 实时流量统计（网站总流量[pv]、网站独立访客数[uv]
```
思路：过滤出'pv'的日志，map（'pv',1）-->分组开窗--->sum(1)
统计uv，AggregateFunction使用布隆过滤器去重,并且merge方法不实现
```
- 按渠道做分析 <br>
```
思路：给渠道打上标签，做分组，然后计算count
```

###**spark streaming**
- 实时日活
```
思路：通过redis去重，先去查redis中是否存在，存在的过滤出该批次，再在同一批次中过滤相同的设备id的记录（按登录时间排序，取最小的），剩余的存入redis
```
- 实时更新orderInfo信息
```
思路: 通过maxwell把mysql中的改变的业务数据导入到kafka，通过配置文件中配置namespace_%{database}%_%{table}%来对数据做分流，用sparkstream对数据做加密处理，字符串截取替换"*",再写入Phoenix或者clickhouse
```
- 实时预警（同一设备，5分钟内三次及以上用不同账号登录并领取优惠劵，并且在登录到领劵过程中没有浏览商品。同时达到以上要求则产生一条预警日志。 同一设备，每分钟只记录一次预警。）
```
思路：开5min的窗口，按设备id分组，把优惠券事件的，用户id的次数放到集合中，判断是否有点击页面的事件，最后，判断集合的次数&&是否有点击事件
```
- 当日新增付费用户首单分析（按地区（用户性别、用户年龄段）统计当日新增付费用户首单平均消费）
```
思路：取查Phoenix，看有没有用户id，没有的话，在再同一批次中去重，最后再查Phoenix中的维表，和过滤后的做组合（join），再存入clickhouse、Phoenix做即席查询数据源  
```  
- 爬虫指标
```
   某个IP，5分钟内的关键页面访问总量
   某个IP，5分钟内的UA种类数统计
```
- 某个IP，5分钟内的关键页面最短访问间隔
```
思路：获取规则，匹配request的关键页面，判断当前是否是关键页面，在获取当前url的时候和ip加关键页面，返回((ip, 关键页面)， 时间)，分组取value，对时间做转换，然后排序获取时间差，最后返回最小时间间隔
```
- 某个IP，5分钟内小于最短访问间隔（自设）的关键页面查询次数
```
实现思路同上，只不过加上相关阈值做判断即可
```
- 某个IP，5分钟内关键页面的访问次数的Cookie数

###**离线指标**





     

